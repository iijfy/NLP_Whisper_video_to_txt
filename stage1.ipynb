{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¬ AI ê¸°ë°˜ ì˜ìƒ ì»¨í…ì¸  ë²ˆì—­ ìë™í™” í”„ë¡œì íŠ¸ 1ë‹¨ê³„<br><br>\n",
        "\n",
        "####1ï¸âƒ£ í”„ë¡œì íŠ¸ ë°°ê²½ ë° ëª©í‘œ\n",
        "ë³¸ í”„ë¡œì íŠ¸ëŠ” ë“œë¼ë§ˆ ì œì‘ì‚¬ ë° ë°°ê¸‰ì‚¬ê°€ ì½˜í…ì¸  ìˆ˜ì¶œ ì‹œ ê²ªëŠ” ìˆ˜ì‘ì—… ë²ˆì—­ ë° ì¬ì œì‘ ë¹„ìš© ë¬¸ì œë¥¼ ì¸ê³µì§€ëŠ¥(AI)ì„ í†µí•´ í•´ê²°í•˜ê³ ì ê¸°íšë˜ì—ˆìŠµë‹ˆë‹¤.  \n",
        "- `ì·¨ì§€`  \n",
        "ì½˜í…ì¸  ìˆ˜ì¶œì„ ìœ„í•œ ì–¸ì–´ í˜„ì§€í™”(ë²ˆì—­) ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ë†’ì€ ë¹„ìš©ê³¼ ì¸ë ¥ ì†Œëª¨ë¥¼ AI ì†”ë£¨ì…˜ìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.\n",
        "- `ë¹„ì¦ˆë‹ˆìŠ¤ ì ì¬ë ¥`   \n",
        "AI ê¸°ë°˜ ìë™ ë²ˆì—­ ì‹œìŠ¤í…œ êµ¬ì¶•ì„ í†µí•´ í•œêµ­ì½˜í…ì¸ ì§„í¥ì›(KOCCA)ê³¼ ê°™ì€ ìœ ê´€ ê¸°ê´€ê³¼ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ì  ì œíœ´ ë° í˜‘ë ¥ ëª¨ë¸ì„ ì°½ì¶œí•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ í™•ì¸í•˜ê³ ì ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤.<br><br>  \n",
        "\n",
        "####2ï¸âƒ£ í…ŒìŠ¤íŠ¸ í™˜ê²½ ë° ì˜ìƒ ë°ì´í„°\n",
        "AI íŒŒì´í”„ë¼ì¸ì˜ ì„±ëŠ¥ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ ì‹¤ì œ ìƒì—… ë“œë¼ë§ˆì˜ í´ë¦½ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.\n",
        "- ì˜ìƒ: ë””ì¦ˆë‹ˆí”ŒëŸ¬ìŠ¤ ì˜¤ë¦¬ì§€ë„ ë“œë¼ë§ˆ \"ì¡°ê°ë„ì‹œ\"\n",
        "- ê¸¸ì´: ì´ 4ë¶„ 2ì´ˆ ë¶„ëŸ‰ì˜ í´ë¦½ì„ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.\n",
        "- íŠ¹ì§•: í•œêµ­ì–´ì™€ ì˜ì–´ê°€ í˜¼ì¬ëœ í™˜ê²½ì„ ê°€ì •í•˜ì—¬ ëª¨ë¸ì˜ ë‹¤êµ­ì–´ ì²˜ë¦¬ ëŠ¥ë ¥ì„ ì¤‘ì ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í–ˆìŠµë‹ˆë‹¤.<br><br>\n",
        "\n",
        "####3ï¸âƒ£ ìŒì„± ì¸ì‹(ASR) ëª¨ë¸ ì„ ì • - Automatic Speech Recognition\n",
        "ë³µí•©ì ì¸ ì–¸ì–´ í™˜ê²½ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ OpenAIì˜ ë‹¤êµ­ì–´ ASR ëª¨ë¸ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤.  \n",
        "ğŸŸ¡ ëª¨ë¸: `openai/whisper-Large`\n",
        "- ëª¨ë¸ê°œìš”: OpenAIê°€ ê°œë°œí•œ ìŒì„±ì¸ì‹ / ë²ˆì—­ëª¨ë¸ ê³„ì—´  \n",
        "- í•µì‹¬êµ¬ì¡°: Transformer ê¸°ë°˜ì˜ ì¸ì½”ë”-ë””ì½”ë” ì•„í‚¤í…ì³\n",
        "- í•™ìŠµë°ì´í„°: ì•¼ 68ë§Œì‹œê°„ ë¶„ëŸ‰ì˜ ë©€í‹°ì–¸ì–´ ìŒì„±ë°ì´í„°ë¡œ í•™ìŠµ\n",
        "- ì„ ì •ì´ìœ : ë“œë¼ë§ˆ ì˜ìƒì— í•œêµ­ì–´ì™€ ì˜ì–´ê°€ í˜¼ì¬ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ë‹¨ì¼ ì–¸ì–´ì „ìš©ëª¨ë¸ì´ ì•„ë‹Œ ë‹¤êµ­ì–´(Multilingual)ì§€ì›ì´ í•„ìˆ˜ì ì´ì—ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.<br><br>\n",
        "\n",
        "####4ï¸âƒ£ ê¸°ê³„ ë²ˆì—­(NMT) ëª¨ë¸ ì„ ì • - Neural Machine Translation\n",
        "ì˜ì–´ë¡œ ì¸ì‹ëœ ìŒì„±ì„ ìµœì¢… ê²°ê³¼ë¬¼ ì–¸ì–´ì¸ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê¸° ìœ„í•œ ëª¨ë¸ì…ë‹ˆë‹¤.  \n",
        "ğŸŸ¡ ëª¨ë¸: `Helsinki-NLP/opus-mt-tc-big-en-ko`  \n",
        "- ëª¨ë¸ê°œìš”: ì˜ì–´â”í•œêµ­ì–´ ë°©í–¥ì˜ ì „ë¬¸ ê¸°ê³„ ë²ˆì—­ëª¨ë¸\n",
        "- í•µì‹¬êµ¬ì¡°: Transformer-Big ì•„í‚¤í…ì³ ê¸°ë°˜\n",
        "- í•™ìŠµë°ì´í„°: OPUS ë¼ëŠ” ëŒ€ê·œëª¨ ë³‘ë ¬ ë§ë­‰ì¹˜(ì›ë¬¸/ë²ˆì—­ ìŒ) ê¸°ë°˜ìœ¼ë¡œ, íŠ¹íˆ ì˜ì–´-í•œêµ­ì–´ ë³‘ë ¬ ë°ì´í„°ê°€ ë§ìŒ.\n",
        "- ì„ ì •ì´ìœ : ì˜ì–´â”í•œêµ­ì–´ ë²ˆì—­ ì§€ì›ì´ ëª…í™•í•˜ê³ , Hugging Face ì˜ pipeline('translation',...)í•¨ìˆ˜ì™€ ì¦‰ì‹œ ì—°ë™ê°€ëŠ¥, ë¬´ë£Œ ê³µê°œëª¨ë¸ì´ë©°, ì½”ë©í™˜ê²½ì—ì„œ êµ¬ë™ì‹œ ë¦¬ì†ŒìŠ¤ ë¶€ë‹´ì´ ì ì–´ íš¨ìœ¨ì ì¸ ì‹¤í—˜ì´ ê°€ëŠ¥í•¨.<br><br>  \n",
        "\n",
        "####5ï¸âƒ£ ìµœì¢… íŒŒì´í”„ë¼ì¸ êµ¬ì¡° ë° ëª¨ë¸ ì„ íƒ ì´ìœ \n",
        "í˜„ì¬ êµ¬ì¶•ëœ ìë§‰ ìƒì„± íŒŒì´í”„ë¼ì¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¡œ êµ¬ì„±ë˜ë©°, ëª¨ë¸ì€ ì´ êµ¬ì¡°ì— ìµœì í™” ë˜ì–´ ì„ íƒí•¨.\n",
        "- ASR(Whisper): ì˜ìƒì˜ ìŒì„±ì„ ì¸ì‹í•˜ì—¬ í•œêµ­ì–´ëŠ” í•œêµ­ì–´ë¡œ, ì˜ì–´ëŠ” ì˜ì–´ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•¨.\n",
        "- ì–¸ì–´ë¶„ë¥˜: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì¤‘ í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤.\n",
        "- ë²ˆì—­(OPUS-MT): ì˜ì–´ í…ìŠ¤íŠ¸ë§Œ ì„ ë³„í•˜ì—¬ í•œêµ­ì–´ë¡œ ë²ˆì—­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
        "- ìµœì¢… í†µí•©: Whisperê°€ ì¸ì‹í•œ ì›ë˜ì˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì™€ OPUS-MTê°€ ë²ˆì—­í•œ ì˜ì–´ì˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ í•©ì³ ìµœì¢… ìë§‰(ëª¨ë‘ í•œêµ­ì–´)ì„ ì™„ì„±<br><br>\n",
        "\n",
        "####6ï¸âƒ£ ìµœì¢… íŒŒì´í”„ë¼ì¸ íë¦„\n",
        "1. ì˜ìƒì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ(mp4â”wav)  \n",
        "2. Whisper ë¡œ ì‹œê°„ ì •ë³´ì™€ í•¨ê»˜ ìŒì„±ì¸ì‹  \n",
        "3. ì¸ì‹ëœ í…ìŠ¤íŠ¸ ì¤‘ ì˜ì–´ ëŒ€ì‚¬ë§Œ ê³¨ë¼ í•œêµ­ì–´ë¡œ ë²ˆì—­  \n",
        "4. ëª¨ë“  í•œêµ­ì–´ í…ìŠ¤íŠ¸ì™€ ì‹œê°„ ì •ë³´ë¥¼ ì¡°í•©í•˜ì—¬ SRT ìë§‰íŒŒì¼ ìƒì„±  \n",
        "5. FFmpeg ë¥¼ ì´ìš©í•´ ì˜ìƒì— ìë§‰ì„ ìµœì¢… ì ìš©.<br><br>\n"
      ],
      "metadata": {
        "id": "arYu2UuuNfSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### â‘  í™˜ê²½ ì„¸íŒ…(ë¼ì´ë¸ŒëŸ¬ë¦¬, GPU, ë“œë¼ì´ë¸Œ, í°íŠ¸)\n",
        "##### - íŒ¨í‚¤ì§€ ì„¤ì¹˜ & í—ˆê¹…í˜ì´ìŠ¤ ë¡œê·¸ì¸ & ê¸°ë³¸ import"
      ],
      "metadata": {
        "id": "dZsI3mpeQ0gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"huggingface_hub\" \"transformers\" \"accelerate\" \"sentencepiece\" -q\n",
        "\"\"\"\n",
        "UëŠ” Upgrade: ì´ë¯¸ ìˆìœ¼ë©´ ë²„ì „ ì˜¬ë¦¬ê¸°\n",
        "accelerate : gpu/cpu íš¨ìœ¨ì²˜ë¦¬\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Secretsì— ì €ì¥ëœ HF_TOKEN ê°’ì„ ê°€ì ¸ì™€ login í•¨ìˆ˜ì— ì „ë‹¬\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(token=hf_token)\n",
        "\n",
        "# ë˜ëŠ”, í† í°ì´ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •ë˜ë©´ login()ë§Œ í˜¸ì¶œí•´ë„ ë©ë‹ˆë‹¤.\n",
        "# import os\n",
        "# os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "# login()\n",
        "\n",
        "import torch\n",
        "import textwrap  # ê¸´ ë¬¸ìì—´ì„ ì¼ì • ê¸¸ì´ë¡œ ì˜ë¼ì£¼ëŠ” ìœ í‹¸\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "\"\"\"\n",
        "AutoModelForSpeechSeq2Seq : ìŒì„±ì…ë ¥->ì‹œí€€ìŠ¤(í…ìŠ¤íŠ¸) ì¶œë ¥ ëª¨ë¸ ìë™ë¡œë”\n",
        "AutoProessor : ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ + í† í¬ë‚˜ì´ì €ë¥¼ í•œë²ˆì— ë¬¶ì–´ì£¼ëŠ” ê°ì²´\n",
        "pipeline: ì‘ì—…ë³„ë¡œ ëª¨ë¸+í† í¬ë‚˜ì´ì €ë¥¼ ê°„í¸í•˜ê²Œ ë¬¶ì–´ì„œ ì“°ëŠ” ë˜í¼, asríŒŒì´í”„ë¼ì¸ì´ë‚˜ ë²ˆì—­íŒŒì´í”„ë¼ì¸ê°™ì´ ì‰½ê²Œ ì“¸ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì–´ë–¤ í‹€.\n",
        "\"\"\"\n",
        "\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU name:', torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "id": "Ok9HLLk_oPMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### - ffmpeg ì„¤ì¹˜ & êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸"
      ],
      "metadata": {
        "id": "q8ClKEuZ1xKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -q install ffmpeg\n",
        "\n",
        "#ffmpeg : ì˜ìƒ/ì˜¤ë””ì–´ ì²˜ë¦¬ íˆ´\n",
        "#apt-get : ë¦¬ëˆ…ìŠ¤ ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ë§¤ë‹ˆì €(Ubuntu)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "k6iabil5uOLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### - ë°°ë¯¼ í°íŠ¸ ì„¤ì¹˜(í•œë‚˜ì²´ í”„ë¡œ)\n",
        "\n"
      ],
      "metadata": {
        "id": "-9imitGM16yO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë“œë¼ì´ë¸Œì— ì—…ë¡œë“œí•œ í°íŠ¸ ê²½ë¡œ (ë„¤ ë“œë¼ì´ë¸Œ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)\n",
        "baemin_font_src = \"/content/drive/MyDrive/fonts/BMHANNAPro.ttf\"\n",
        "\n",
        "if not os.path.exists(baemin_font_src):\n",
        "    raise FileNotFoundError(f\"ë°°ë¯¼ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {baemin_font_src}\")\n",
        "\n",
        "# ë¦¬ëˆ…ìŠ¤ ê³µìš© í°íŠ¸ í´ë”ì— ë³µì‚¬\n",
        "!mkdir -p /usr/share/fonts/truetype/baemin  #mkdir -p : í´ë” ì—†ìœ¼ë©´ ë§Œë“¤ê³ , ìˆìœ¼ë©´ ì¡°ìš©íˆ ë„˜ì–´ê°.\n",
        "!cp \"$baemin_font_src\" /usr/share/fonts/truetype/baemin/  #cp : íŒŒì¼ë³µì‚¬\n",
        "\n",
        "# í°íŠ¸ ìºì‹œ ìƒˆë¡œê³ ì¹¨\n",
        "!fc-cache -fv\n",
        "\n",
        "# ì„¤ì¹˜ëœ í°íŠ¸ ì´ë¦„ í™•ì¸(ffmpeg force_styleì—ì„œ ì‚¬ìš©)\n",
        "!fc-list | grep -i hanna\n"
      ],
      "metadata": {
        "id": "1cALo0-4IvPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### â‘¡ ë“œë¼ë§ˆ ì˜ìƒì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ(mp4 â” wav)\n",
        "##### - mp4 â” 16kHz mono wav"
      ],
      "metadata": {
        "id": "azRBtvbJTHEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/drive/MyDrive/project_src/drama_themanipulated_4m2s.mp4\"\n",
        "\n",
        "if not os.path.exists(video_path):\n",
        "    raise FileNotFoundError(f'ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ìˆ˜ê°€ ì—†ì–´ìš”: {video_path}')\n",
        "\n",
        "# Whisperê°€ ì¢‹ì•„í•˜ëŠ” í¬ë§·, ffmpeg ë¡œ ì˜¤ë””ì˜¤ ì¶”ì¶œ(16kHz, mono wav)\n",
        "audio_filename = 'audio_16k_mono.wav'\n",
        "\n",
        "!ffmpeg -i '{video_path}' -vn -acodec pcm_s16le -ar 16000 -ac 1 \"{audio_filename}\"\n",
        "\n",
        "# -i : input ì…ë ¥íŒŒì¼\n",
        "# -vn : video no. ì¦‰, ë¹„ë””ì˜¤ëŠ” ë²„ë¦¬ê³  ì˜¤ë””ì˜¤ë§Œ ì¶”ì¶œ\n",
        "# -acodec pcm_s16le : ì˜¤ë””ì˜¤ ì½”ë± í˜•ì‹ ì§€ì •(16-bit PCM)\n",
        "# -ar 16000 : sample rate = 16kHz\n",
        "# -ac 1 : mono(1ì±„ë„)"
      ],
      "metadata": {
        "id": "f3WB9L9LUBHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### â‘¢ Whisperë¡œ ìŒì„± -> í…ìŠ¤íŠ¸(+íƒ€ì„ìŠ¤íƒ¬í”„)\n"
      ],
      "metadata": {
        "id": "waRP0OdPVQkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'openai/whisper-large'\n",
        "\"\"\"\n",
        "small\n",
        "medium\n",
        "large ì‹¤í—˜ í›„ ìµœì¢… Large ì‚¬ìš©\n",
        "\"\"\"\n",
        "device = 0 if torch.cuda.is_available() else 'cpu'\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32 #GPU ìˆì„ë•Œ 16ë¹„íŠ¸ ì“°ë©´ ë³´í†µ ë” ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ ì ˆì•½\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ(safetensors ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ì½ì–´ì˜¤ê¸°)\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(  #í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ model_idì— í•´ë‹¹í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œí•˜ê³  ë¡œë“œ\n",
        "    model_id,\n",
        "    torch_dtype=torch_dtype,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "# ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ + í† í¬ë‚˜ì´ì €ë¥¼ ë¬¶ì–´ì£¼ëŠ” Processor\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "\n",
        "# ASR íŒŒì´í”„ë¼ì¸ ì •ì˜\n",
        "asr_pipe = pipeline(\n",
        "    'automatic-speech-recognition', #ASR #ë¯¸ë¦¬ ì •ì˜ëœ íƒœìŠ¤í¬ ì´ë¦„ì„ ì£¼ë©´ ë‚´ë¶€ì—ì„œ ì•Œì•„ì„œ ëª¨ë¸, í† í¬ë‚˜ì´ì €, ì „ì²˜ë¦¬ê¸°ë¥¼ ë¬¶ì–´ì¤Œ.\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor, #ì˜¤ë””ì˜¤ë¥¼ ëª¨ë¸ ì…ë ¥ìš© ìˆ«ì ë²¡í„°ë¡œ ë°”ê¿”ì¤Œ.\n",
        "    torch_dtype = torch_dtype,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "# ë‹¨ì–´ ë‹¨ìœ„ íƒ€ì„ìŠ¤íƒ¬í”„ ì–»ê¸°\n",
        "asr_result = asr_pipe(\n",
        "    audio_filename,\n",
        "    return_timestamps=\"word\"\n",
        ")\n",
        "\"\"\"\n",
        "ğŸŸª word-level timestamps\n",
        "asr_result[\"chunks\"]ì—,\n",
        "{\"text\": \"ë‹¨ì–´\", \"timestamp\": (start, end)} í˜•íƒœë¡œ ë“¤ì–´ì˜´.\n",
        "\"\"\"\n",
        "\n",
        "print(asr_result.keys())\n",
        "print('ì „ì²´ í…ìŠ¤íŠ¸ ì˜ˆì‹œ:', asr_result['text'][:200])\n",
        "print('word-chunk ì˜ˆì‹œ 10ê°œ:' , asr_result['chunks'][:10])\n"
      ],
      "metadata": {
        "id": "OlQ_6doiVBlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### â‘£ \"word\"ë“¤ì„ ë“œë¼ë§ˆ ìë§‰ ë¸”ëŸ­ìœ¼ë¡œ ë¬¶ê¸°\n"
      ],
      "metadata": {
        "id": "ucmOwqLR3aTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### - word chunks â” ì´ˆë²Œ ìë§‰ ë¸”ëŸ­ ë¦¬ìŠ¤íŠ¸(raw_subs)"
      ],
      "metadata": {
        "id": "vPWQRZ713i0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ğŸ’¬ group_words_to_subtitlesëŠ” ë‹¨ìˆœíˆ ê¸€ì ìˆ˜/ì‹œê°„/ì¹¨ë¬µ ê¸°ì¤€ìœ¼ë¡œë§Œ ë¬¶ì–´ì„œ,\n",
        "ë¬¸ë²•ì´ë‚˜ ì¡°ì‚¬ê°€ ëŠê¸°ì§€ ì•Šë„ë¡ â€œì–¸ì–´ì â€ìœ¼ë¡œ ì´í•´í•˜ì§„ ëª»í•˜ëŠ”ê²Œ í•œê³„ë‹¤. ì—¬ê¸°ì„œ Whisper X ë˜ëŠ” ì‚¬ëŒì†ì´ í•„ìš”í•˜ë‹¤.\n",
        "\"\"\"\n",
        "\n",
        "def group_words_to_subtitles(\n",
        "    word_chunks,\n",
        "    max_chars: int = 50,      # í•œ ìë§‰ ë¸”ëŸ­ì— í—ˆìš©í•  ìµœëŒ€ ê¸€ì ìˆ˜\n",
        "    max_duration: float = 6.0, # í•œ ìë§‰ ë¸”ëŸ­ì˜ ìµœëŒ€ ì‹œê°„(ì´ˆ)\n",
        "    max_gap: float = 1.5,     # ë‹¨ì–´ ì‚¬ì´ ì¹¨ë¬µì´ max_gapë³´ë‹¤ í¬ë©´ ìƒˆ ìë§‰\n",
        "    min_word_dur: float = 0.05, # ë„ˆë¬´ ì§§ì€ ë‹¨ì–´(ë…¸ì´ì¦ˆ) ìµœì†Œ ê¸¸ì´\n",
        "):\n",
        "    \"\"\"\n",
        "    Whisperê°€ ì¤€ word-level chunk ë“¤ì„ 'ìë§‰ ë¸”ëŸ­'ìœ¼ë¡œ ë¬¶ì–´ì£¼ëŠ” í•¨ìˆ˜.\n",
        "\n",
        "    word_chunks: asr_result[\"chunks\"]\n",
        "      ê° ì›ì†ŒëŠ” {\"text\": \"...\", \"timestamp\": (start, end)} í˜•íƒœ.\n",
        "\n",
        "    ë°˜í™˜: ìë§‰ ë¸”ëŸ­ ë¦¬ìŠ¤íŠ¸\n",
        "      [{\"start\": float, \"end\": float, \"text\": str}, ...]\n",
        "    \"\"\"\n",
        "    subs = []              # ìµœì¢… ìë§‰ ë¸”ëŸ­ ë¦¬ìŠ¤íŠ¸\n",
        "    cur_words = []         # í˜„ì¬ ìë§‰ ë¸”ëŸ­ì— ë“¤ì–´ê°ˆ ë‹¨ì–´ë“¤\n",
        "    cur_start = None       # í˜„ì¬ ë¸”ëŸ­ì˜ ì‹œì‘ ì‹œê°„\n",
        "    last_end = None        # í˜„ì¬ ë¸”ëŸ­ì—ì„œ ë§ˆì§€ë§‰ ë‹¨ì–´ì˜ ë ì‹œê°„\n",
        "    last_word = None       # (text, s, e) ì§ì „ ë‹¨ì–´ (ì¤‘ë³µ ì œê±°ìš©)\n",
        "\n",
        "    # ë¬¸ì¥ ì¤‘ê°„ì—ì„œ ìƒˆ ë©ì–´ë¦¬ë¥¼ ì‹œì‘í•˜ê³  ì‹¶ì€ ì ‘ì†ì–´ë“¤\n",
        "    CONNECTIVE_START = {\"ê·¸ë˜ì„œ\", \"ê·¸ë¦¬ê³ \", \"ê·¼ë°\", \"ê·¸ëŸ¬ë‹ˆê¹Œ\"}\n",
        "\n",
        "    for ch in word_chunks:\n",
        "        w = ch.get(\"text\") or \"\"\n",
        "        w = w.strip()\n",
        "        if not w:\n",
        "            continue\n",
        "\n",
        "        ts = ch.get(\"timestamp\")\n",
        "        if not ts:\n",
        "            continue\n",
        "        s, e = ts\n",
        "        if s is None or e is None:\n",
        "            continue\n",
        "\n",
        "        dur = e - s\n",
        "        # 0ì´ˆ ì´í•˜(ì—­ì „) êµ¬ê°„ì´ë‚˜ ë„ˆë¬´ ì§§ì€ êµ¬ê°„ì€ ë…¸ì´ì¦ˆë¡œ ë³´ê³  ìŠ¤í‚µ\n",
        "        if dur <= 0:\n",
        "            continue\n",
        "        if dur < min_word_dur:\n",
        "            continue\n",
        "\n",
        "        # ê±°ì˜ ê°™ì€ ì‹œê°„ì— ê°™ì€ ë‹¨ì–´ê°€ ë°˜ë³µ ì¸ì‹ë˜ë©´ í•œ ë²ˆë§Œ ì‚¬ìš©\n",
        "        if last_word is not None:\n",
        "            last_text, last_s, last_e = last_word\n",
        "            if (\n",
        "                w == last_text and\n",
        "                abs(s - last_s) < 0.02 and\n",
        "                abs(e - last_e) < 0.02\n",
        "            ):\n",
        "                # ê°™ì€ ë‹¨ì–´ê°€ ê°™ì€ ìœ„ì¹˜ì— ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¨ ê²ƒìœ¼ë¡œ ë³´ê³  ê±´ë„ˆëœ€\n",
        "                continue\n",
        "\n",
        "        # ì²« ë‹¨ì–´ë©´ ìƒˆ ìë§‰ ì‹œì‘\n",
        "        if cur_start is None:\n",
        "            cur_start = s\n",
        "            last_end = e\n",
        "            cur_words = [w]\n",
        "            last_word = (w, s, e)\n",
        "            continue\n",
        "\n",
        "        # ì´ ë‹¨ì–´ë¥¼ ë¶™ì˜€ì„ ë•Œë¥¼ ê°€ì •í•˜ê³  ì—¬ëŸ¬ ê¸°ì¤€ìœ¼ë¡œ ê²€ì‚¬\n",
        "        candidate_text = \" \".join(cur_words + [w])\n",
        "        duration = e - cur_start      # í˜„ì¬ ë¸”ëŸ­ì˜ ì´ ê¸¸ì´\n",
        "        gap = s - last_end            # ì§ì „ ë‹¨ì–´ì™€ì˜ ì‹œê°„ ê°„ê²©\n",
        "\n",
        "        need_new_sub = False\n",
        "\n",
        "        # 1) ê¸€ì ìˆ˜ ë„ˆë¬´ ê¸¸ë©´ ëŠê¸°\n",
        "        if len(candidate_text) > max_chars:\n",
        "            need_new_sub = True\n",
        "\n",
        "        # 2) ë¸”ëŸ­ ì‹œê°„ì´ ë„ˆë¬´ ê¸¸ë©´ ëŠê¸°\n",
        "        if duration > max_duration:\n",
        "            need_new_sub = True\n",
        "\n",
        "        # 3) ë‹¨ì–´ ì‚¬ì´ ì¹¨ë¬µì´ ë„ˆë¬´ ê¸¸ë©´ ëŠê¸°\n",
        "        if gap > max_gap:\n",
        "            need_new_sub = True\n",
        "\n",
        "        # 4) 'ê·¸ë˜ì„œ/ê·¸ë¦¬ê³ /ê·¼ë°/ê·¸ëŸ¬ë‹ˆê¹Œ' ê°™ì€ ì ‘ì†ì–´ëŠ”\n",
        "        #    ì• ë¸”ëŸ­ì´ ì–´ëŠ ì •ë„ ê¸¸ë©´ ìƒˆ ìë§‰ìœ¼ë¡œ ë³´ë‚´ê¸°\n",
        "        if w in CONNECTIVE_START and len(\" \".join(cur_words)) > 20:\n",
        "            need_new_sub = True\n",
        "\n",
        "        if need_new_sub:\n",
        "            # í˜„ì¬ ë¸”ëŸ­ì´ ë„ˆë¬´ ì§§ìœ¼ë©´(1ë‹¨ì–´ì§œë¦¬) ê·¸ëƒ¥ ì´ì–´ë¶™ì´ê³  ê³„ì† ì§„í–‰\n",
        "            if len(cur_words) == 1:\n",
        "                cur_words.append(w)\n",
        "                last_end = e\n",
        "            else:\n",
        "                # ì§€ê¸ˆê¹Œì§€ ëª¨ì¸ ë‹¨ì–´ë“¤ë¡œ ìë§‰ í•˜ë‚˜ í™•ì •\n",
        "                sub_text = \" \".join(cur_words)\n",
        "                subs.append({\n",
        "                    \"start\": cur_start,\n",
        "                    \"end\": last_end,\n",
        "                    \"text\": sub_text,\n",
        "                })\n",
        "                # ìƒˆ ë¸”ëŸ­ ì‹œì‘\n",
        "                cur_start = s\n",
        "                cur_words = [w]\n",
        "                last_end = e\n",
        "        else:\n",
        "            # ë¸”ëŸ­ ê³„ì† ì´ì–´ë¶™ì´ê¸°\n",
        "            cur_words.append(w)\n",
        "            last_end = e\n",
        "\n",
        "        last_word = (w, s, e)\n",
        "\n",
        "    # ë§ˆì§€ë§‰ ë©ì–´ë¦¬ ì²˜ë¦¬\n",
        "    if cur_words:\n",
        "        sub_text = \" \".join(cur_words)\n",
        "        subs.append({\n",
        "            \"start\": cur_start,\n",
        "            \"end\": last_end,\n",
        "            \"text\": sub_text,\n",
        "        })\n",
        "\n",
        "    # ìë§‰ ë¸”ëŸ­ ë ˆë²¨ì—ì„œ í•œ ë²ˆ ë” ì •ë¦¬\n",
        "    cleaned = []\n",
        "    for sub in subs:\n",
        "        start = float(sub[\"start\"])\n",
        "        end = float(sub[\"end\"])\n",
        "        text = sub[\"text\"].strip()\n",
        "\n",
        "        # 0ì´ˆì§œë¦¬ ë¸”ëŸ­ì€ ë²„ë¦¬ê¸° (start == end)\n",
        "        if end <= start:\n",
        "            continue\n",
        "\n",
        "        # ë°”ë¡œ ì• ìë§‰ê³¼ ì™„ì „íˆ ê°™ì€ í…ìŠ¤íŠ¸ + ì‹œê°„ë„ ê±°ì˜ ì´ì–´ì ¸ ìˆìœ¼ë©´ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°\n",
        "        if (\n",
        "            cleaned and\n",
        "            cleaned[-1][\"text\"] == text and\n",
        "            abs(cleaned[-1][\"end\"] - start) < 0.1\n",
        "        ):\n",
        "            cleaned[-1][\"end\"] = end\n",
        "            continue\n",
        "\n",
        "        cleaned.append({\n",
        "            \"start\": start,\n",
        "            \"end\": end,\n",
        "            \"text\": text,\n",
        "        })\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "\n",
        "word_chunks = asr_result[\"chunks\"]\n",
        "raw_subs = group_words_to_subtitles(\n",
        "    word_chunks,\n",
        "    max_chars=50,\n",
        "    max_duration=6.0,\n",
        "    max_gap=1.5,\n",
        "    min_word_dur=0.05,\n",
        ")\n",
        "\n",
        "print(\"ì´ˆë²Œ ìë§‰ ë¸”ëŸ­ ê°œìˆ˜:\", len(raw_subs))\n",
        "print(\"ì˜ˆì‹œ 30ê°œ:\")\n",
        "for sub in raw_subs[:30]:\n",
        "    print(sub)\n"
      ],
      "metadata": {
        "id": "iG7nRHHqlz6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### â‘¤ ì˜ì–´ë§Œ ë²ˆì—­ â” í•œêµ­ì–´ í†µí•© + í›„ì²˜ë¦¬(postprocess)\n",
        "##### - ë²ˆì—­ íŒŒì´í”„ë¼ì¸ + í›„ì²˜ë¦¬ í•¨ìˆ˜ + processed_chunks ìƒì„±"
      ],
      "metadata": {
        "id": "dx-zBSWzXeok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ì˜ì–´ â” í•œêµ­ì–´ ë²ˆì—­ íŒŒì´í”„ë¼ì¸\n",
        "translator = pipeline(\n",
        "    \"translation\",\n",
        "    model=\"Helsinki-NLP/opus-mt-tc-big-en-ko\",\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "# ë²ˆì—­ í…ŒìŠ¤íŠ¸\n",
        "test = translator(\"I am testing English to Korean translation.\")[0][\"translation_text\"]\n",
        "print(\"í…ŒìŠ¤íŠ¸ ë²ˆì—­:\", test)\n",
        "\n",
        "\n",
        "# ë²ˆì—­ í•„ìš” ì—¬ë¶€ íŒë‹¨\n",
        "def contains_hangul(text: str) -> bool:  #ë¬¸ìì—´ì— í•œê¸€(ê°€~í£)ì´ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ 'True'\n",
        "    for ch in text:\n",
        "        if \"\\uac00\" <= ch <= \"\\ud7a3\":\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def needs_translation(text: str) -> bool:\n",
        "\n",
        "    if contains_hangul(text):   # í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì´ë¯¸ í•œêµ­ì–´ê°€ ì„ì—¬ ìˆë‹¤ê³  ë³´ê³  ë²ˆì—­í•˜ì§€ ì•ŠìŒ.\n",
        "        return False\n",
        "\n",
        "    if any(c.isalpha() for c in text):   # í•œê¸€ì€ ì—†ëŠ”ë° ì•ŒíŒŒë²³ë§Œ ìˆìœ¼ë©´ ì˜ì–´ë¼ê³  ë³´ê³  ë²ˆì—­\n",
        "        return True\n",
        "    return False                        # ê·¸ ì™¸(ìˆ«ì, ê¸°í˜¸): ë²ˆì—­ ì•ˆí•¨\n",
        "\n",
        "\n",
        "# ASRì´ ì´ìƒí•˜ê²Œ ì¸ì‹í•œ í‘œí˜„ í›„ì²˜ë¦¬\n",
        "def postprocess_ko_text(text: str) -> str:\n",
        "    fixes = {\n",
        "        #âœ…small\n",
        "        # \"ë¹¨ë¼ê³ \": \"íŒ”ë¡œìš°\",\n",
        "        # \"í˜¼ë²•\" : \"í—Œë²•\",\n",
        "        # \"ì˜ë¯¸ì¸\" : \"ì˜ë¬¸\",\n",
        "        # \"ê°–ë‹¤ëŠ”\" : \"ê°”ë‹¤ëŠ”\",\n",
        "        # \"í˜ì´ì§€ì•„ì´\" : \"í”¼í•´ìì˜\",\n",
        "        # \"í–‰ì •êµ¬ì—ê²Œ\" : \"í–‰ì •êµ¬ì—­ì—\",\n",
        "        # \"ì„œìœ \" : \"ì†Œìš”\",\n",
        "        # \"ë¹„ë˜ë©´\" : \"ë¹„ëŒ€ë©´\",\n",
        "        # \"ê°ì‹œ\" : \"ê°ì‹\",\n",
        "        # \"ì´ì…\" : \"ì´ì˜ìˆ\",\n",
        "        # \"ì§€í‚¤ì§€\" : \"ì°íˆì§€\",\n",
        "        # \"ì‚¬ê°ì§€ ë“¤ìœ¼ë©´\" : \"ì‚¬ê°ì§€ëŒ€ë¡œë§Œ\",\n",
        "        # \"í”¼ê³ ì¹˜ê²Œ ì´í•´\" : \"í”¼ê³ ì¸¡ì˜ ì´ì˜ì—\",\n",
        "        # \"í”¼ì§€\" : \"í”¼ì˜ì\",\n",
        "        # \"ì•Œë ¤ê°€ì§€ê³ \" : \"í•˜ì…”ê°€ì§€ê³ \",\n",
        "        #âœ…medium\n",
        "        # \"ì˜ë¯¸ì \" : \"ì˜ë¬¸ì \",\n",
        "        # \"PHI\" : \"í”¼í•´ì\",\n",
        "        # \"ì² ë¬¸í¬\" : \"ì Šì€ì´\",\n",
        "        # \"í•˜ì…”ì„œìš”\" : \"í•˜ì…”ê°€ì§€ê³ ìš”\",\n",
        "        # \"ê°ì‹œ\" : \"ê°ì‹\",\n",
        "        # \"ì˜ˆ ìˆìŠµë‹ˆë‹¤\" : \"ì´ì˜ ìˆìŠµë‹ˆë‹¤\"\n",
        "        # \"ì§€í‚¤ì§€\" : \"ì°íˆì§€\",\n",
        "        # \"ì´í•´\" : \"ì´ì˜ì—\",\n",
        "        # \"í•˜ë ¤ê°€ì§€ê³ \" : \"í•˜ì…”ê°€ì§€ê³ \",\n",
        "        #âœ…large\n",
        "        \"PHI\" : \"í”¼í•´ìì˜\",\n",
        "        \"í”¼ê³  ì¸¡ì— ì˜í•´\" : \"í”¼ê³ ì¸¡ì˜ ì´ì˜ì—\",\n",
        "        \"í•˜ë ¤ê°€ì§€ê³ \" : \"í•˜ì…”ê°€ì§€ê³ \",\n",
        "    }\n",
        "    for wrong, correct in fixes.items():\n",
        "        text = text.replace(wrong, correct)\n",
        "    return text\n",
        "\n",
        "\n",
        "def merge_subtitles_by_sentence(chunks,\n",
        "                               max_merged_chars: int = 60,\n",
        "                               max_time_gap: float = 1.2):\n",
        "    \"\"\"\n",
        "    processed_chunksë¥¼ ë°›ì•„ì„œ,\n",
        "    ë¬¸ì¥ ì¤‘ê°„ì— ì˜ë¦° ê²ƒ ê°™ì€ ìë§‰ë“¤ì„ ë‹¤ì‹œ í•©ì³ì£¼ëŠ” í›„ì²˜ë¦¬.\n",
        "\n",
        "    - ì´ì „ ìë§‰ì´ ë¬¸ì¥ ë(., ?, !, 'ë‹¤.', 'ìš”.') ë“±ìœ¼ë¡œ ëë‚˜ì§€ ì•Šê³ \n",
        "    - ë‘ ìë§‰ ì‚¬ì´ì˜ ì‹œê°„ ê°„ê²©ì´ ë„ˆë¬´ í¬ì§€ ì•Šìœ¼ë©°\n",
        "    - ë‘˜ì„ í•©ì³¤ì„ ë•Œ ê¸€ì ìˆ˜ê°€ max_merged_chars ì´í•˜ì´ë©´\n",
        "\n",
        "    â†’ í•˜ë‚˜ì˜ ìë§‰ ë©ì–´ë¦¬ë¡œ ë³‘í•©í•œë‹¤.\n",
        "    \"\"\"\n",
        "    if not chunks:\n",
        "        return []\n",
        "\n",
        "    merged = [chunks[0].copy()]\n",
        "\n",
        "    for cur in chunks[1:]:\n",
        "        prev = merged[-1]\n",
        "        prev_text = prev[\"text\"].rstrip()\n",
        "        cur_text = cur[\"text\"].strip()\n",
        "\n",
        "        sentence_endings = (\"ë‹¤.\", \"ìš”.\", \"ê¹Œ?\", \"ëƒ?\", \"ë‹ˆë‹¤.\", \".\", \"?\", \"!\")\n",
        "        if prev_text.endswith(sentence_endings):\n",
        "            merged.append(cur.copy())\n",
        "            continue\n",
        "\n",
        "        gap = cur[\"start\"] - prev[\"end\"]\n",
        "        if gap > max_time_gap:\n",
        "            merged.append(cur.copy())\n",
        "            continue\n",
        "\n",
        "        candidate = prev_text + \" \" + cur_text\n",
        "        if len(candidate) > max_merged_chars:\n",
        "            merged.append(cur.copy())\n",
        "            continue\n",
        "\n",
        "        # ìœ„ ì¡°ê±´ ë‹¤ í†µê³¼í•˜ë©´ ë³‘í•©\n",
        "        prev[\"end\"] = cur[\"end\"]\n",
        "        prev[\"text\"] = candidate\n",
        "\n",
        "    return merged\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def refine_timing(chunks,\n",
        "                  global_delay: float = 0.2,  # ì „ì²´ ìë§‰ì„ 0.2ì´ˆ ëŠ¦ê²Œ\n",
        "                  min_gap: float = 0.15):     # ë¸”ëŸ­ë¼ë¦¬ ë„ˆë¬´ ë¶™ì–´ ìˆìœ¼ë©´ ì¡°ê¸ˆ ë„ìš°ê¸°\n",
        "    \"\"\"\n",
        "    - ê° ìë§‰ ë¸”ëŸ­ì˜ start/endë¥¼ global_delay ë§Œí¼ í†µì§¸ë¡œ ë°€ì–´ì¤Œ\n",
        "    - ì„œë¡œ ë„ˆë¬´ ë¶™ì–´ ìˆìœ¼ë©´ min_gap ë§Œí¼ ë²Œë ¤ì¤Œ\n",
        "    - ê¸¸ì´(= end - start)ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ\n",
        "    \"\"\"\n",
        "    if not chunks:\n",
        "        return []\n",
        "\n",
        "    refined = []\n",
        "    last_end = None\n",
        "\n",
        "    for ch in chunks:\n",
        "        start = float(ch[\"start\"]) + global_delay\n",
        "        end = float(ch[\"end\"]) + global_delay\n",
        "        text = ch[\"text\"]\n",
        "\n",
        "        # ì´ì „ ìë§‰ê³¼ ê²¹ì¹˜ë©´ ê°™ì´ ì¡°ê¸ˆ ë’¤ë¡œ ë°€ê¸°\n",
        "        if last_end is not None and start < last_end + min_gap:\n",
        "            shift = (last_end + min_gap) - start\n",
        "            start += shift\n",
        "            end += shift\n",
        "\n",
        "        refined.append({\n",
        "            \"start\": start,\n",
        "            \"end\": end,\n",
        "            \"text\": text,\n",
        "        })\n",
        "        last_end = end\n",
        "\n",
        "    return refined\n",
        "\n",
        "\n",
        "# raw_subì— ë²ˆì—­ + í›„ì²˜ë¦¬ ì ìš©\n",
        "processed_chunks = []\n",
        "\n",
        "for sub in raw_subs:\n",
        "    text = sub[\"text\"].strip()\n",
        "    if not text:\n",
        "        continue\n",
        "\n",
        "    # ì˜ì–´ë§Œ ë²ˆì—­\n",
        "    if needs_translation(text):\n",
        "        tr = translator(text)[0][\"translation_text\"]\n",
        "        ko_text = tr.strip()\n",
        "    else:\n",
        "        ko_text = text\n",
        "\n",
        "    # í›„ì²˜ë¦¬ë¡œ ASR ì˜¤íƒ€ êµì •\n",
        "    ko_text = postprocess_ko_text(ko_text)\n",
        "\n",
        "    processed_chunks.append({\n",
        "        \"start\": sub[\"start\"],\n",
        "        \"end\": sub[\"end\"],\n",
        "        \"text\": ko_text,\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "# ë¬¸ì¥ ë‹¨ìœ„ ë³‘í•©\n",
        "#processed_chunks = merge_subtitles_by_sentence(\n",
        "#    processed_chunks,\n",
        "#    max_merged_chars=50,\n",
        "#    max_time_gap=1.2,\n",
        "#)\n",
        "\n",
        "# íƒ€ì´ë° ì •ë¦¬: ì˜¤ë””ì˜¤ ê¸°ì¤€ + ì¼ì • ë”œë ˆì´ë§Œ ì ìš©\n",
        "processed_chunks = refine_timing(\n",
        "    processed_chunks,\n",
        "    global_delay=0.2,\n",
        "    min_gap=0.15,\n",
        ")\n",
        "\n",
        "\n",
        "print(\"ì²˜ë¦¬ëœ ìë§‰ ë¸”ëŸ­ ìˆ˜:\", len(processed_chunks))\n",
        "print(\"ì˜ˆì‹œ 30ê°œ:\")\n",
        "for ch in processed_chunks[:30]:\n",
        "    print(ch)\n",
        "\n"
      ],
      "metadata": {
        "id": "cyUO-sgmmrkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ch in processed_chunks:\n",
        "    if 100 <= ch[\"start\"] <= 150 or 100 <= ch[\"end\"] <= 150:\n",
        "        print(ch)\n"
      ],
      "metadata": {
        "id": "GHsC-PIaKKli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### â‘¥ SRT ìƒì„± + ë“œë¼ì´ë¸Œì— ì €ì¥\n",
        "##### - SRT ì‹œê°„ ë¬¸ìì—´, ê¸´ ë¬¸ì¥ ì¤„ë°”ê¿ˆ, íŒŒì¼ì €ì¥"
      ],
      "metadata": {
        "id": "bRRirEehZtO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ˆ(float) â” SRT ì‹œê°„ ë¬¸ìì—´ 'HH:MM:SS,mmm'\n",
        "def sec_to_srt_time(sec: float) -> str:\n",
        "    if sec is None:\n",
        "        sec = 0.0\n",
        "\n",
        "    millis = int(round(sec * 1000))\n",
        "\n",
        "    hours = millis // (3600 * 1000)  # //ëŠ” ì •ìˆ˜ ë‚˜ëˆ—ì…ˆ -> ëª«ë§Œ ì–»ê³  ë‚˜ë¨¸ì§€ëŠ” ë²„ë¦¬ê¸°\n",
        "    millis = millis % (3600 * 1000)  # %ëŠ” ë‚˜ë¨¸ì§€ë§Œ\n",
        "\n",
        "    minutes = millis // (60 * 1000)\n",
        "    millis = millis % (60 * 1000)\n",
        "\n",
        "    seconds = millis // 1000\n",
        "    millis = millis % 1000\n",
        "\n",
        "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d},{millis:03d}\"  # 'HH:MM:SS,mmm' í˜•ì‹ìœ¼ë¡œ ë°˜í™˜\n",
        "\n",
        "\n",
        "# ê¸´ ìë§‰ ë¬¸ì¥ì„ ì˜í™” ìë§‰ì²˜ëŸ¼ 1~2ì¤„ ì •ë„ë¡œ ì˜ë¼ì£¼ëŠ” í•¨ìˆ˜\n",
        "def wrap_subtitle_text(text: str,\n",
        "                       width: int = 25,  #width: ëŒ€ëµ í•œ ì¤„ë‹¹ í—ˆìš© ê¸€ì ìˆ˜\n",
        "                       max_lines: int = 2) -> list[str]:  #max_lines: í—ˆìš© ì¤„ ìˆ˜(ë³´í†µ ì˜í™” ìë§‰ì€ 2ì¤„ ì´í•˜)\n",
        "\n",
        "    cleaned = \" \".join(text.split()) #ê³µë°±ì •ë¦¬ (ì—¬ëŸ¬ê³µë°±->í•œ ì¹¸)\n",
        "    lines = textwrap.wrap(cleaned, width=width) # ìë§‰ í•œì¤„ë‹¹ ëŒ€ëµ 25ì ì •ë„ë¡œ ì ë‹¹íˆ ì˜ë¼ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
        "\n",
        "    if len(lines) <= max_lines:    #ì¤„ ìˆ˜ê°€ max_lines ì´í•˜ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
        "        return lines\n",
        "\n",
        "    #ì¤„ ìˆ˜ê°€ ë„ˆë¬´ ë§ìœ¼ë©´, ì•ì˜ (max_lines-1)ì¤„ + ë‚˜ë¨¸ì§€ë¥¼ ë§ˆì§€ë§‰ ì¤„ì— í•©ì¹˜ê¸°\n",
        "    head = lines[:max_lines-1]\n",
        "    tail = \" \".join(lines[max_lines-1:]) # \" \".join(...) : ë‹¤ì‹œ í•œì¹¸ì”©ë§Œ ë¶™ì´ê¸° -> ì¤‘ë³µ ê³µë°± ì •ë¦¬\n",
        "    return head + [tail]\n",
        "\n",
        "\n",
        "# processed_chunks í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ í‘œì¤€ SRT í˜•ì‹ì˜ íŒŒì¼ë¡œ ì €ì¥\n",
        "def save_srt(chunks, srt_filename=\"drama_ko_themanupulated.srt\"):\n",
        "    lines = []\n",
        "    idx = 1\n",
        "\n",
        "    for ch in chunks:\n",
        "        text = ch['text'].strip()\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        start = sec_to_srt_time(ch['start'])\n",
        "        end = sec_to_srt_time(ch['end'])\n",
        "\n",
        "        # âœ… ì—¬ê¸°ì„œ ê¸´ ë¬¸ì¥ì„ 1~2ì¤„ë¡œ ìë¥´ê¸°\n",
        "        wrapped_lines = wrap_subtitle_text(text, width=22, max_lines=2)\n",
        "\n",
        "        lines.append(str(idx))\n",
        "        lines.append(f\"{start} --> {end}\")\n",
        "        # í•œ ìë§‰ ë¸”ëŸ­ ì•ˆì—ì„œë„ ì—¬ëŸ¬ ì¤„ ë„£ì–´ë„ ë¨\n",
        "        for wline in wrapped_lines:\n",
        "            lines.append(wline)\n",
        "        lines.append('')\n",
        "\n",
        "        idx += 1\n",
        "\n",
        "    srt_content = '\\n'.join(lines)\n",
        "\n",
        "    with open(srt_filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(srt_content)\n",
        "\n",
        "    print(f'SRT íŒŒì¼ ì €ì¥ ì™„ë£Œ: {srt_filename}')\n",
        "\n",
        "\n",
        "# ì‹¤ì œ SRT íŒŒì¼ ì €ì¥\n",
        "srt_local = \"drama_ko_themanupulated.srt\"\n",
        "save_srt(processed_chunks, srt_local)\n",
        "\n",
        "\n",
        "# êµ¬ê¸€ë“œë¼ì´ë¸Œì— ë°±ì—…\n",
        "dst_path = '/content/drive/MyDrive/project_src/drama_ko_themanupulated.srt'\n",
        "shutil.copy('drama_ko_themanupulated.srt', dst_path)\n",
        "\n",
        "print('ë“œë¼ì´ë¸Œ ì €ì¥ì™„ë£Œ:', dst_path)\n"
      ],
      "metadata": {
        "id": "F2yntXmyaMkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### â‘¦ ìë§‰ì„ ì˜ìƒì— ì…íˆê¸° & ìŠ¤íƒ€ì¼ ì§€ì •\n",
        "##### - ffmpegë¡œ SRTë¥¼ ì˜ìƒì— êµ¬ì›Œ ë„£ê¸°"
      ],
      "metadata": {
        "id": "AvNspJTjdPsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë“œë¼ì´ë¸Œì— ìˆëŠ” ì›ë³¸ ì˜ìƒ ê²½ë¡œ (ì•ì—ì„œ ì¼ë˜ê±° ê·¸ëŒ€ë¡œ)\n",
        "video_path = '/content/drive/MyDrive/project_src/drama_themanipulated_4m2s.mp4'\n",
        "\n",
        "if not os.path.exists(video_path):\n",
        "    raise FileNotFoundError(f\"ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}\")\n",
        "\n",
        "# ë°©ê¸ˆ ë§Œë“  SRT\n",
        "srt_path = \"drama_ko_themanupulated.srt\"\n",
        "if not os.path.exists(srt_path):\n",
        "    raise FileNotFoundError(f\"SRT íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {srt_path}\")\n",
        "\n",
        "\n",
        "# ìë§‰ì´ ì…í˜€ì§„ ê²°ê³¼ ì˜ìƒì„ ë“œë¼ì´ë¸Œì— ì €ì¥í•  ê²½ë¡œ\n",
        "output_video_path = \"/content/drive/MyDrive/project_src/drama_themanipulated_4m2s_ko_sub.mp4\"\n",
        "\n",
        "print(\"ì›ë³¸ ì˜ìƒ:\", video_path)\n",
        "print(\"ì‚¬ìš©í•  SRT:\", srt_path)\n",
        "print(\"ì¶œë ¥ ì˜ìƒ:\", output_video_path)\n",
        "\n",
        "# í°íŠ¸ ì´ë¦„ì€ fc-listì—ì„œ í™•ì¸í•œ ê°’ ì‚¬ìš©\n",
        "style = (\n",
        "    \"FontName=BM Hanna Air,\"  # ë°°ë¯¼ í•œë‚˜ì²´ Air\n",
        "    \"FontSize=25,\"            # ê¸€ì í¬ê¸°\n",
        "    \"PrimaryColour=&H00FFFFFF,\"   # í°ìƒ‰ (BGR + ì•ŒíŒŒ)\n",
        "    \"OutlineColour=&H00000000,\"   # ê²€ì • ì™¸ê³½ì„ \n",
        "    \"Outline=2,\"               # ì™¸ê³½ì„  ë‘ê»˜\n",
        "    \"BorderStyle=1,\"           # ì¼ë°˜ í…Œë‘ë¦¬\n",
        "    \"Shadow=0,\"                # ê·¸ë¦¼ì ì—†ìŒ\n",
        "    \"Alignment=2,\"             # 2 = ì•„ë˜ ê°€ìš´ë°\n",
        "    \"MarginV=40\"               # í™”ë©´ ì•„ë˜ì—ì„œ ìœ„ë¡œ 40px ì—¬ë°±\n",
        ")\n",
        "style = \"\".join(style)\n",
        "\n",
        "\n",
        "# ffmpegë¡œ í•˜ë“œì„œë¸Œ(ì˜ìƒì— ìë§‰ êµ½ê¸°)\n",
        "!ffmpeg -y -i \"$video_path\" \\\n",
        "  -vf \"subtitles=$srt_path:force_style='$style'\" \\\n",
        "  -c:a copy \"$output_video_path\"\n",
        "\n",
        "\n",
        "print(\"ì €ì¥ì™„ë£Œ!\")\n"
      ],
      "metadata": {
        "id": "QGope3sveRRE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
